{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 3\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. Submissions of single students will not be accepted! Please use the Forum in case of doubt in order to find team mates!\n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Tuesday, 21st of January 23:55h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples.\n",
    "\n",
    "Do not forget to specify the (First_name, Last_name, student_id (matrikelnummer)) of all contributing students in the jupyter notebook here:\n",
    "\n",
    "Student 1: `Sourabh, Zanwar, 391923`\n",
    "\n",
    "Student 2: `Malek, Alhelwany, 391558`\n",
    "\n",
    "Student 3: `Makua, Bernal, 392224`\n",
    "\n",
    "Student 4: `Tobias, Biele, 344413`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a \"kaggle\"-like competition within the class. You will be given a task and you will have to find a solution that maximizes the accuracy of some unknown test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "You are provided with a set of reviews written on the Yelp platform. Your task is to predict the ratings (between 1 and 5) from the review text. \n",
    "We will evaluate your solution with respect to accuracy, i.e., for how many reviews in the hold out set did your solution predicted the correct rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to come up with a classification model, which gives the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset here - https://drive.google.com/file/d/1Wm2nkOzwj7PSRY9q_FY2TgjqEuKhykmv/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have only provided a the training set and we will evaluate your model on a held out test set. Of course you can sample a small validation dataset from the given training set. You can start from a base architecture and then add modifications to it. Apart from the notebook, you will have to submit a 1-page report describing how you ended up with your final network design (what did you try, what were the results, why did you do which modifications, etc...). At the minimum, you should try at least one CNN and one RNN-based architecture and test some modifications for potential improvement.\n",
    "\n",
    "You can reuse the codes from exercises and previous home assignments. Please indicate on your submission also if you are fine with us sharing your solutions with your fellow students of this course if you make it to one of the top solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout,Conv1D,MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the dataset\n",
    "col_names = ['Rating', 'Review']\n",
    "dataset = pd.read_csv('data/train.csv', names=col_names, header=None)\n",
    "del col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = SnowballStemmer('english')\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "del review,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing test for CNN and LSTM\n",
    "\n",
    "X = corpus\n",
    "y = dataset.iloc[0:len(corpus), 0].values\n",
    "#X_train, X_val, y_train, y_val = train_test_split(corpus, y, test_size = 0.10, random_state = 0)\n",
    "\n",
    "def decrement(list):\n",
    "    return [x - 1 for x in list]\n",
    "#Using Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=100,padding=\"post\")\n",
    "\n",
    "num_classes = 5\n",
    "y = to_categorical(decrement(y), num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourabhzanwar/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 520000 samples, validate on 130000 samples\n",
      "Epoch 1/3\n",
      "520000/520000 [==============================] - 2576s 5ms/step - loss: 1.0378 - accuracy: 0.5437 - val_loss: 0.9066 - val_accuracy: 0.6057\n",
      "Epoch 2/3\n",
      "520000/520000 [==============================] - 2580s 5ms/step - loss: 0.9351 - accuracy: 0.5924 - val_loss: 0.8841 - val_accuracy: 0.6191\n",
      "Epoch 3/3\n",
      "520000/520000 [==============================] - 2539s 5ms/step - loss: 0.8823 - accuracy: 0.6194 - val_loss: 0.8895 - val_accuracy: 0.6172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x153193b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using CNN over LSTM\n",
    "\n",
    "#Building the model\n",
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size, 100, input_length=100))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=4))\n",
    "model_conv.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_conv.add(Dense(5, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Training the model\n",
    "model_conv.fit(X, y, batch_size=64, epochs=3, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior implemented models:\n",
    "Below are the different models that we tried to implement, before finalising the model with CNN over LSTM.All the models are working, but we selected the one because it had the highest accuracy of all, also the time required by that mmodel is less as compared to the LSTM(5ms/step for CNN and 6ms/step for LSTM). For LSTM the validation accuracy was around 61% after 3 epochs, and for classification the accuracy was 38%. For the LSTM, the input data for train is the same data used for the above CNN model, but for the classification we modify the data using CountVectorizer, and select top 1000 features from the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourabhzanwar/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 520000 samples, validate on 130000 samples\n",
      "Epoch 1/3\n",
      "520000/520000 [==============================] - 2982s 6ms/step - loss: 1.2332 - accuracy: 0.4401 - val_loss: 0.9168 - val_accuracy: 0.6060\n",
      "Epoch 2/3\n",
      "520000/520000 [==============================] - 2993s 6ms/step - loss: 0.9451 - accuracy: 0.5872 - val_loss: 0.8903 - val_accuracy: 0.6162\n",
      "Epoch 3/3\n",
      "520000/520000 [==============================] - 2890s 6ms/step - loss: 0.8838 - accuracy: 0.6196 - val_loss: 0.8869 - val_accuracy: 0.6178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x154b9ba50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using LSTM\n",
    "\n",
    "#Building the model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size, 100, input_length=100))\n",
    "model_lstm.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(5, activation=\"softmax\"))\n",
    "model_lstm.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Training Model\n",
    "model_lstm.fit(X, y, batch_size=64, epochs=3, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.38382307692307693\n"
     ]
    }
   ],
   "source": [
    "#Using Naive Bayes Classification\n",
    "\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "X_nb = cv.fit_transform(corpus).toarray()\n",
    "y_nb = dataset.iloc[0:len(corpus), 0].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_nb, y_nb, test_size = 0.20)\n",
    "\n",
    "#Training the classifier\n",
    "classifierNB = GaussianNB()\n",
    "classifierNB.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_predNB = classifierNB.predict(X_val)\n",
    "\n",
    "#Check Accuracy :\n",
    "a = 0\n",
    "for i in range(len(y_predNB)):\n",
    "    if y_predNB[i]==y_val[i]:\n",
    "        a = a + 1\n",
    "accuracyNB = (a/len(y_predNB))\n",
    "del a\n",
    "print('Accuracy is : ', accuracyNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
